{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/angel/Downloads/ProjetI&A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we import the libraries which will help us building the cnn, the first two are for building the neural network while the two left will help for the convolution part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 32 features dectectors and each detector have a 3x3 size\n",
    "relu is used to remove the linearity, we need to specify the format of images (64x64 pixels + RGB channels)\n",
    "stride default value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(64,64,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value of the stride is 2\n",
    "Goal : reduce the size of the feature maps in order to reduce the processing while keeping the important info (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add another layer to our cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32,kernel_size=(3,3),activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name of this step implies, we are literally going to flatten our pooled feature map into a column like in the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our typical artificial neural network (Ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile our cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact we currently have 8000 pictures in our dataset(which is not enough) we must enrich our dataset, for that we are going to use a keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From /home/angel/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 37s 146ms/step - loss: 0.6592 - acc: 0.6076 - val_loss: 0.6380 - val_acc: 0.6335\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.5947 - acc: 0.6819 - val_loss: 0.5510 - val_acc: 0.7355\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.5546 - acc: 0.7183 - val_loss: 0.5203 - val_acc: 0.7485\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.5324 - acc: 0.7355 - val_loss: 0.5144 - val_acc: 0.7425\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 36s 145ms/step - loss: 0.5087 - acc: 0.7530 - val_loss: 0.4818 - val_acc: 0.7675\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 36s 144ms/step - loss: 0.4907 - acc: 0.7614 - val_loss: 0.4831 - val_acc: 0.7630\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.4750 - acc: 0.7701 - val_loss: 0.4877 - val_acc: 0.7565\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.4637 - acc: 0.7772 - val_loss: 0.5154 - val_acc: 0.7720\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.4483 - acc: 0.7909 - val_loss: 0.4732 - val_acc: 0.7860\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 36s 142ms/step - loss: 0.4347 - acc: 0.7967 - val_loss: 0.4513 - val_acc: 0.7820\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.4300 - acc: 0.8014 - val_loss: 0.4537 - val_acc: 0.7920\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.4181 - acc: 0.8024 - val_loss: 0.4387 - val_acc: 0.7920\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.4096 - acc: 0.8130 - val_loss: 0.4762 - val_acc: 0.7875\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.3965 - acc: 0.8196 - val_loss: 0.4583 - val_acc: 0.8025\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 35s 139ms/step - loss: 0.3914 - acc: 0.8193 - val_loss: 0.4697 - val_acc: 0.7830\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.3847 - acc: 0.8235 - val_loss: 0.4622 - val_acc: 0.7950\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.3755 - acc: 0.8241 - val_loss: 0.4747 - val_acc: 0.7750\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 34s 138ms/step - loss: 0.3701 - acc: 0.8321 - val_loss: 0.4363 - val_acc: 0.8065\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 34s 138ms/step - loss: 0.3533 - acc: 0.8384 - val_loss: 0.4415 - val_acc: 0.8075\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 35s 141ms/step - loss: 0.3613 - acc: 0.8366 - val_loss: 0.4278 - val_acc: 0.8125\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 37s 150ms/step - loss: 0.3492 - acc: 0.8424 - val_loss: 0.4242 - val_acc: 0.8115\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 35s 140ms/step - loss: 0.3328 - acc: 0.8515 - val_loss: 0.4423 - val_acc: 0.8080\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 34s 137ms/step - loss: 0.3305 - acc: 0.8538 - val_loss: 0.4647 - val_acc: 0.8075\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 35s 138ms/step - loss: 0.3271 - acc: 0.8544 - val_loss: 0.4397 - val_acc: 0.8160\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 36s 143ms/step - loss: 0.3318 - acc: 0.8539 - val_loss: 0.4560 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd186355780>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=(8000/32),\n",
    "        epochs=25,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=(2000/32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cat.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-14049eb3c4a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    102\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    103\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cnn/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cat.png'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('cat.png', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result=classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result [0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)\n",
    "\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
